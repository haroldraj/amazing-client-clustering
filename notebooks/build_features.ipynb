{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eafa2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe0da4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERIM_PATH = \"../.data/interim\"\n",
    "PROCESSED_PATH = \"../.data/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1382902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c74de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85185cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2dbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9f903f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64e37e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_features_from_chunk_main(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger.info(\"Converting event_time to datetime and extracting date\")\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "    df['date'] = df['event_time'].dt.date\n",
    "\n",
    "    logger.info(\"Filtering purchase events\")\n",
    "    purchase_df = df[df['event_type'] == 'purchase']\n",
    "\n",
    "    logger.info(\"Grouping by user_id\")\n",
    "    grouped = df.groupby('user_id')\n",
    "\n",
    "    logger.info(\"Aggregating user features\")\n",
    "    features = grouped.agg(\n",
    "        count_view=('event_type', lambda x: (x == 'view').sum()),\n",
    "        count_cart=('event_type', lambda x: (x == 'cart').sum()),\n",
    "        count_purchase=('event_type', lambda x: (x == 'purchase').sum()),\n",
    "        unique_sessions=('user_session', pd.Series.nunique),\n",
    "        fav_main_category=('main_category', lambda x: x.mode(\n",
    "        ).iloc[0] if not x.mode().empty else 'unknown'),\n",
    "        fav_sub_category=('sub_category', lambda x: x.mode(\n",
    "        ).iloc[0] if not x.mode().empty else 'unknown'),\n",
    "        first_event=('event_time', 'min'),\n",
    "        last_event=('event_time', 'max'),\n",
    "        active_days=('date', 'nunique')\n",
    "    )\n",
    "\n",
    "    logger.info(\"Computing total and average purchase amounts\")\n",
    "    purchase_stats = purchase_df.groupby('user_id')['price'].agg(['sum', 'mean']).rename(\n",
    "        columns={'sum': 'total_spent', 'mean': 'avg_purchase_price'}\n",
    "    )\n",
    "\n",
    "    logger.info(\"Merging features with purchase statistics\")\n",
    "    features = features.join(purchase_stats, how='left')\n",
    "\n",
    "    logger.info(\"Calculating recency_days\")\n",
    "    latest_date = features['last_event'].max()\n",
    "    features['recency_days'] = (latest_date - features['last_event']).dt.days\n",
    "\n",
    "    features[[\"total_spent\", \"avg_purchase_price\"]] = features[[\n",
    "        \"total_spent\", \"avg_purchase_price\"]].fillna(0)\n",
    "\n",
    "    logger.info(\"Reordering final feature columns\")\n",
    "    features = features[\n",
    "        [\n",
    "            \"count_view\", \"count_cart\", \"count_purchase\",\n",
    "            \"unique_sessions\", \"fav_main_category\", \"fav_sub_category\",\n",
    "            \"active_days\", \"recency_days\",\n",
    "            \"total_spent\", \"avg_purchase_price\",\n",
    "            \"first_event\", \"last_event\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    logger.info(\"Feature extraction complete\")\n",
    "    return features.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_features_main():\n",
    "    logger.info(\"Starting user feature extraction for all chunks\")\n",
    "    os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "\n",
    "    files = sorted([f for f in os.listdir(\n",
    "        INTERIM_PATH) if f.endswith(\".parquet\")])\n",
    "    logger.info(\"Found %d chunks to process\", len(files))\n",
    "\n",
    "    for i, file in enumerate(tqdm(files, desc=\"Building user features from chunks\")):\n",
    "        chunk_path = os.path.join(INTERIM_PATH, file)\n",
    "        logger.info(\"Reading chunk %s\", file)\n",
    "        df = pd.read_parquet(chunk_path)\n",
    "\n",
    "        logger.info(\"Building features from chunk %s\", i)\n",
    "        user_features = build_user_features_from_chunk_main(df)\n",
    "\n",
    "        output_path = os.path.join(\n",
    "            PROCESSED_PATH, f\"user_features_chunk_{i}.parquet\")\n",
    "        user_features.to_parquet(output_path, index=False)\n",
    "        logger.info(\"Saved user features for chunk %s to %s\", i, output_path)\n",
    "\n",
    "    logger.info(\"All chunks processed and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97eee4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 16:07:13,659 | INFO | __main__ | Starting user feature extraction for all chunks\n",
      "2025-05-31 16:07:13,661 | INFO | __main__ | Found 17 chunks to process\n",
      "Building user features from chunks:   0%|          | 0/17 [00:00<?, ?it/s]2025-05-31 16:07:13,733 | INFO | __main__ | Reading chunk cleaned_chunk_0.parquet\n",
      "2025-05-31 16:07:24,840 | INFO | __main__ | Building features from chunk 0\n",
      "2025-05-31 16:07:24,840 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 16:07:31,780 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 16:07:33,280 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 16:07:33,281 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 16:35:15,336 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 16:35:15,410 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 16:35:15,594 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 16:35:15,715 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 16:35:15,818 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 16:35:17,321 | INFO | __main__ | Saved user features for chunk 0 to ../.data/processed\\user_features_chunk_0.parquet\n",
      "Building user features from chunks:   6%|▌         | 1/17 [28:03<7:28:57, 1683.59s/it]2025-05-31 16:35:17,323 | INFO | __main__ | Reading chunk cleaned_chunk_1.parquet\n",
      "2025-05-31 16:35:29,204 | INFO | __main__ | Building features from chunk 1\n",
      "2025-05-31 16:35:29,205 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 16:35:35,864 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 16:35:37,350 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 16:35:37,351 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 17:02:20,502 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 17:02:20,586 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 17:02:20,782 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 17:02:20,892 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 17:02:21,001 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 17:02:22,556 | INFO | __main__ | Saved user features for chunk 1 to ../.data/processed\\user_features_chunk_1.parquet\n",
      "Building user features from chunks:  12%|█▏        | 2/17 [55:08<6:52:18, 1649.26s/it]2025-05-31 17:02:22,558 | INFO | __main__ | Reading chunk cleaned_chunk_10.parquet\n",
      "2025-05-31 17:02:25,450 | INFO | __main__ | Building features from chunk 2\n",
      "2025-05-31 17:02:25,451 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 17:02:26,892 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 17:02:27,218 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 17:02:27,219 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 17:10:09,668 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 17:10:09,681 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 17:10:09,740 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 17:10:09,776 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 17:10:09,809 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 17:10:10,264 | INFO | __main__ | Saved user features for chunk 2 to ../.data/processed\\user_features_chunk_2.parquet\n",
      "Building user features from chunks:  18%|█▊        | 3/17 [1:02:56<4:18:56, 1109.74s/it]2025-05-31 17:10:10,266 | INFO | __main__ | Reading chunk cleaned_chunk_11.parquet\n",
      "2025-05-31 17:10:20,452 | INFO | __main__ | Building features from chunk 3\n",
      "2025-05-31 17:10:20,453 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 17:10:26,965 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 17:10:28,452 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 17:10:28,453 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 17:36:44,895 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 17:36:44,980 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 17:36:45,163 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 17:36:45,272 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 17:36:45,381 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 17:36:46,847 | INFO | __main__ | Saved user features for chunk 3 to ../.data/processed\\user_features_chunk_3.parquet\n",
      "Building user features from chunks:  24%|██▎       | 4/17 [1:29:33<4:42:05, 1301.94s/it]2025-05-31 17:36:46,849 | INFO | __main__ | Reading chunk cleaned_chunk_12.parquet\n",
      "2025-05-31 17:36:56,644 | INFO | __main__ | Building features from chunk 4\n",
      "2025-05-31 17:36:56,645 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 17:37:02,273 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 17:37:03,523 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 17:37:03,524 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 18:03:24,255 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 18:03:24,326 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 18:03:24,516 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 18:03:24,624 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 18:03:24,731 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 18:03:26,215 | INFO | __main__ | Saved user features for chunk 4 to ../.data/processed\\user_features_chunk_4.parquet\n",
      "Building user features from chunks:  29%|██▉       | 5/17 [1:56:12<4:41:50, 1409.19s/it]2025-05-31 18:03:26,217 | INFO | __main__ | Reading chunk cleaned_chunk_13.parquet\n",
      "2025-05-31 18:03:37,182 | INFO | __main__ | Building features from chunk 5\n",
      "2025-05-31 18:03:37,183 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 18:03:43,813 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 18:03:45,247 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 18:03:45,247 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 18:32:41,448 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 18:32:41,509 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 18:32:41,706 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 18:32:41,830 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 18:32:41,949 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 18:32:43,533 | INFO | __main__ | Saved user features for chunk 5 to ../.data/processed\\user_features_chunk_5.parquet\n",
      "Building user features from chunks:  35%|███▌      | 6/17 [2:25:29<4:40:03, 1527.56s/it]2025-05-31 18:32:43,535 | INFO | __main__ | Reading chunk cleaned_chunk_14.parquet\n",
      "2025-05-31 18:32:53,487 | INFO | __main__ | Building features from chunk 6\n",
      "2025-05-31 18:32:53,488 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 18:32:59,208 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 18:33:00,477 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 18:33:00,478 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 18:59:36,486 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 18:59:36,559 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 18:59:36,741 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 18:59:36,848 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 18:59:36,956 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 18:59:38,454 | INFO | __main__ | Saved user features for chunk 6 to ../.data/processed\\user_features_chunk_6.parquet\n",
      "Building user features from chunks:  41%|████      | 7/17 [2:52:24<4:19:21, 1556.12s/it]2025-05-31 18:59:38,455 | INFO | __main__ | Reading chunk cleaned_chunk_15.parquet\n",
      "2025-05-31 18:59:49,995 | INFO | __main__ | Building features from chunk 7\n",
      "2025-05-31 18:59:49,996 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 18:59:56,606 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 18:59:58,138 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 18:59:58,138 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 19:31:24,964 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 19:31:25,071 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 19:31:25,281 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 19:31:25,405 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 19:31:25,526 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 19:31:27,333 | INFO | __main__ | Saved user features for chunk 7 to ../.data/processed\\user_features_chunk_7.parquet\n",
      "Building user features from chunks:  47%|████▋     | 8/17 [3:24:13<4:10:15, 1668.42s/it]2025-05-31 19:31:27,335 | INFO | __main__ | Reading chunk cleaned_chunk_16.parquet\n",
      "2025-05-31 19:31:37,552 | INFO | __main__ | Building features from chunk 8\n",
      "2025-05-31 19:31:37,554 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 19:31:43,918 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 19:31:45,276 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 19:31:45,277 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 19:54:20,480 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 19:54:20,555 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 19:54:20,706 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 19:54:20,800 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 19:54:20,887 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 19:54:22,134 | INFO | __main__ | Saved user features for chunk 8 to ../.data/processed\\user_features_chunk_8.parquet\n",
      "Building user features from chunks:  53%|█████▎    | 9/17 [3:47:08<3:30:13, 1576.63s/it]2025-05-31 19:54:22,136 | INFO | __main__ | Reading chunk cleaned_chunk_2.parquet\n",
      "2025-05-31 19:54:25,371 | INFO | __main__ | Building features from chunk 9\n",
      "2025-05-31 19:54:25,371 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 19:54:27,047 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 19:54:27,421 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 19:54:27,422 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 20:05:08,756 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 20:05:08,779 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 20:05:08,856 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 20:05:08,903 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 20:05:08,947 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 20:05:09,552 | INFO | __main__ | Saved user features for chunk 9 to ../.data/processed\\user_features_chunk_9.parquet\n",
      "Building user features from chunks:  59%|█████▉    | 10/17 [3:57:55<2:30:28, 1289.76s/it]2025-05-31 20:05:09,554 | INFO | __main__ | Reading chunk cleaned_chunk_3.parquet\n",
      "2025-05-31 20:05:20,713 | INFO | __main__ | Building features from chunk 10\n",
      "2025-05-31 20:05:20,714 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 20:05:27,605 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 20:05:29,015 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 20:05:29,016 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 20:28:48,400 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 20:28:48,448 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 20:28:48,606 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 20:28:48,700 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 20:28:48,799 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 20:28:50,085 | INFO | __main__ | Saved user features for chunk 10 to ../.data/processed\\user_features_chunk_10.parquet\n",
      "Building user features from chunks:  65%|██████▍   | 11/17 [4:21:36<2:12:58, 1329.79s/it]2025-05-31 20:28:50,087 | INFO | __main__ | Reading chunk cleaned_chunk_4.parquet\n",
      "2025-05-31 20:29:01,532 | INFO | __main__ | Building features from chunk 11\n",
      "2025-05-31 20:29:01,532 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 20:29:08,080 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 20:29:09,521 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 20:29:09,522 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 20:50:31,222 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 20:50:31,300 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 20:50:31,462 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 20:50:31,549 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 20:50:31,632 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 20:50:32,898 | INFO | __main__ | Saved user features for chunk 11 to ../.data/processed\\user_features_chunk_11.parquet\n",
      "Building user features from chunks:  71%|███████   | 12/17 [4:43:19<1:50:07, 1321.58s/it]2025-05-31 20:50:32,899 | INFO | __main__ | Reading chunk cleaned_chunk_5.parquet\n",
      "2025-05-31 20:50:36,062 | INFO | __main__ | Building features from chunk 12\n",
      "2025-05-31 20:50:36,063 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 20:50:37,755 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 20:50:38,122 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 20:50:38,123 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 20:59:39,588 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 20:59:39,605 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 20:59:39,674 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 20:59:39,716 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 20:59:39,760 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 20:59:40,284 | INFO | __main__ | Saved user features for chunk 12 to ../.data/processed\\user_features_chunk_12.parquet\n",
      "Building user features from chunks:  76%|███████▋  | 13/17 [4:52:26<1:12:28, 1087.05s/it]2025-05-31 20:59:40,285 | INFO | __main__ | Reading chunk cleaned_chunk_6.parquet\n",
      "2025-05-31 20:59:50,311 | INFO | __main__ | Building features from chunk 13\n",
      "2025-05-31 20:59:50,312 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 20:59:57,531 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 20:59:59,152 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 20:59:59,153 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 21:24:18,687 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 21:24:18,768 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 21:24:18,938 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 21:24:19,035 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 21:24:19,135 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 21:24:20,505 | INFO | __main__ | Saved user features for chunk 13 to ../.data/processed\\user_features_chunk_13.parquet\n",
      "Building user features from chunks:  82%|████████▏ | 14/17 [5:17:06<1:00:17, 1205.81s/it]2025-05-31 21:24:20,507 | INFO | __main__ | Reading chunk cleaned_chunk_7.parquet\n",
      "2025-05-31 21:24:25,125 | INFO | __main__ | Building features from chunk 14\n",
      "2025-05-31 21:24:25,126 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 21:24:27,976 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 21:24:28,643 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 21:24:28,644 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 21:37:35,027 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 21:37:35,057 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 21:37:35,160 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 21:37:35,216 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 21:37:35,271 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 21:37:36,043 | INFO | __main__ | Saved user features for chunk 14 to ../.data/processed\\user_features_chunk_14.parquet\n",
      "Building user features from chunks:  88%|████████▊ | 15/17 [5:30:22<36:04, 1082.14s/it]  2025-05-31 21:37:36,045 | INFO | __main__ | Reading chunk cleaned_chunk_8.parquet\n",
      "2025-05-31 21:37:46,242 | INFO | __main__ | Building features from chunk 15\n",
      "2025-05-31 21:37:46,243 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 21:37:52,825 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 21:37:54,279 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 21:37:54,280 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 22:03:52,028 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 22:03:52,108 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 22:03:52,289 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 22:03:52,404 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 22:03:52,510 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 22:03:54,016 | INFO | __main__ | Saved user features for chunk 15 to ../.data/processed\\user_features_chunk_15.parquet\n",
      "Building user features from chunks:  94%|█████████▍| 16/17 [5:56:40<20:31, 1231.39s/it]2025-05-31 22:03:54,017 | INFO | __main__ | Reading chunk cleaned_chunk_9.parquet\n",
      "2025-05-31 22:04:05,151 | INFO | __main__ | Building features from chunk 16\n",
      "2025-05-31 22:04:05,194 | INFO | __main__ | Converting event_time to datetime and extracting date\n",
      "2025-05-31 22:04:11,689 | INFO | __main__ | Filtering purchase events\n",
      "2025-05-31 22:04:13,235 | INFO | __main__ | Grouping by user_id\n",
      "2025-05-31 22:04:13,237 | INFO | __main__ | Aggregating user features\n",
      "2025-05-31 22:30:48,297 | INFO | __main__ | Computing total and average purchase amounts\n",
      "2025-05-31 22:30:48,362 | INFO | __main__ | Merging features with purchase statistics\n",
      "2025-05-31 22:30:48,548 | INFO | __main__ | Calculating recency_days\n",
      "2025-05-31 22:30:48,658 | INFO | __main__ | Reordering final feature columns\n",
      "2025-05-31 22:30:48,766 | INFO | __main__ | Feature extraction complete\n",
      "2025-05-31 22:30:50,296 | INFO | __main__ | Saved user features for chunk 16 to ../.data/processed\\user_features_chunk_16.parquet\n",
      "Building user features from chunks: 100%|██████████| 17/17 [6:23:36<00:00, 1353.92s/it]\n",
      "2025-05-31 22:30:50,301 | INFO | __main__ | All chunks processed and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "build_all_features_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b0ed537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_user_feature_chunks_aggregated():\n",
    "    logger.info(\"Merging all user feature chunks with user-level aggregation\")\n",
    "\n",
    "    all_chunks = []\n",
    "    files = sorted([\n",
    "        f for f in os.listdir(PROCESSED_PATH)\n",
    "        if f.startswith(\"user_features_chunk_\") and f.endswith(\".parquet\")\n",
    "    ])\n",
    "\n",
    "    for file in tqdm(files, desc=\"Loading feature chunks\"):\n",
    "        path = os.path.join(PROCESSED_PATH, file)\n",
    "        df = pd.read_parquet(path)\n",
    "        all_chunks.append(df)\n",
    "\n",
    "    full_df = pd.concat(all_chunks, ignore_index=True)\n",
    "    logger.info(\"Concatenated shape: %s\", full_df.shape)\n",
    "\n",
    "    # Group by user_id and aggregate numeric features\n",
    "    logger.info(\"Aggregating user features for duplicate user_ids\")\n",
    "    aggregated = full_df.groupby(\"user_id\").agg({\n",
    "        \"count_view\": \"sum\",\n",
    "        \"count_cart\": \"sum\",\n",
    "        \"count_purchase\": \"sum\",\n",
    "        \"unique_sessions\": \"sum\",\n",
    "        \"active_days\": \"sum\",\n",
    "        \"recency_days\": \"min\",  # most recent chunk is better\n",
    "        \"total_spent\": \"sum\",\n",
    "        \"avg_purchase_price\": \"mean\",  # simple avg across chunks\n",
    "        \"first_event\": \"min\",\n",
    "        \"last_event\": \"max\",\n",
    "        \"fav_main_category\": lambda x: x.mode().iloc[0] if not x.mode().empty else \"unknown\",\n",
    "        \"fav_sub_category\": lambda x: x.mode().iloc[0] if not x.mode().empty else \"unknown\"\n",
    "    }).reset_index()\n",
    "\n",
    "    output_path = os.path.join(PROCESSED_PATH, \"user_features.parquet\")\n",
    "    aggregated.to_parquet(output_path, index=False)\n",
    "    logger.info(\"Saved final aggregated user features to %s\", output_path)\n",
    "\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6cd34c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 23:31:20,252 | INFO | __main__ | Merging all user feature chunks with user-level aggregation\n",
      "Loading feature chunks: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n",
      "2025-05-31 23:31:26,067 | INFO | __main__ | Concatenated shape: (36312031, 13)\n",
      "2025-05-31 23:31:26,068 | INFO | __main__ | Aggregating user features for duplicate user_ids\n",
      "2025-06-01 00:53:04,921 | INFO | __main__ | Saved final aggregated user features to ../.data/processed\\user_features.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>count_view</th>\n",
       "      <th>count_cart</th>\n",
       "      <th>count_purchase</th>\n",
       "      <th>unique_sessions</th>\n",
       "      <th>active_days</th>\n",
       "      <th>recency_days</th>\n",
       "      <th>total_spent</th>\n",
       "      <th>avg_purchase_price</th>\n",
       "      <th>first_event</th>\n",
       "      <th>last_event</th>\n",
       "      <th>fav_main_category</th>\n",
       "      <th>fav_sub_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10300217</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-06 06:51:52+00:00</td>\n",
       "      <td>2019-11-06 06:51:52+00:00</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12511517</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-02-24 05:52:05+00:00</td>\n",
       "      <td>2020-03-08 17:23:57+00:00</td>\n",
       "      <td>apparel</td>\n",
       "      <td>shoes.moccasins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22165363</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-01-30 08:17:02+00:00</td>\n",
       "      <td>2020-03-13 05:04:50+00:00</td>\n",
       "      <td>computers</td>\n",
       "      <td>bedroom.bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27396220</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-10 04:21:00+00:00</td>\n",
       "      <td>2020-04-10 04:21:00+00:00</td>\n",
       "      <td>construction</td>\n",
       "      <td>components.faucet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29515875</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-10 02:08:39+00:00</td>\n",
       "      <td>2020-04-23 05:03:44+00:00</td>\n",
       "      <td>furniture</td>\n",
       "      <td>bedroom.bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15639798</th>\n",
       "      <td>649775813</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-30 23:58:54+00:00</td>\n",
       "      <td>2020-04-30 23:58:54+00:00</td>\n",
       "      <td>apparel</td>\n",
       "      <td>shoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15639799</th>\n",
       "      <td>649775850</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-30 23:59:04+00:00</td>\n",
       "      <td>2020-04-30 23:59:04+00:00</td>\n",
       "      <td>furniture</td>\n",
       "      <td>universal.light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15639800</th>\n",
       "      <td>649775918</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-30 23:59:26+00:00</td>\n",
       "      <td>2020-04-30 23:59:26+00:00</td>\n",
       "      <td>apparel</td>\n",
       "      <td>shoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15639801</th>\n",
       "      <td>649775938</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-30 23:59:33+00:00</td>\n",
       "      <td>2020-04-30 23:59:37+00:00</td>\n",
       "      <td>construction</td>\n",
       "      <td>tools.light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15639802</th>\n",
       "      <td>649775983</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-30 23:59:46+00:00</td>\n",
       "      <td>2020-04-30 23:59:46+00:00</td>\n",
       "      <td>apparel</td>\n",
       "      <td>shoes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15639803 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id  count_view  count_cart  count_purchase  unique_sessions  \\\n",
       "0          10300217           1           0               0                1   \n",
       "1          12511517           2           0               0                2   \n",
       "2          22165363          14           0               0               13   \n",
       "3          27396220           1           0               0                1   \n",
       "4          29515875          23           0               0               14   \n",
       "...             ...         ...         ...             ...              ...   \n",
       "15639798  649775813           1           0               0                1   \n",
       "15639799  649775850           1           0               0                1   \n",
       "15639800  649775918           1           0               0                1   \n",
       "15639801  649775938           2           0               0                1   \n",
       "15639802  649775983           1           0               0                1   \n",
       "\n",
       "          active_days  recency_days  total_spent  avg_purchase_price  \\\n",
       "0                   1             9          0.0                 0.0   \n",
       "1                   2             5          0.0                 0.0   \n",
       "2                   9             0          0.0                 0.0   \n",
       "3                   1             6          0.0                 0.0   \n",
       "4                   9             2          0.0                 0.0   \n",
       "...               ...           ...          ...                 ...   \n",
       "15639798            1             0          0.0                 0.0   \n",
       "15639799            1             0          0.0                 0.0   \n",
       "15639800            1             0          0.0                 0.0   \n",
       "15639801            1             0          0.0                 0.0   \n",
       "15639802            1             0          0.0                 0.0   \n",
       "\n",
       "                       first_event                last_event  \\\n",
       "0        2019-11-06 06:51:52+00:00 2019-11-06 06:51:52+00:00   \n",
       "1        2020-02-24 05:52:05+00:00 2020-03-08 17:23:57+00:00   \n",
       "2        2020-01-30 08:17:02+00:00 2020-03-13 05:04:50+00:00   \n",
       "3        2020-04-10 04:21:00+00:00 2020-04-10 04:21:00+00:00   \n",
       "4        2019-11-10 02:08:39+00:00 2020-04-23 05:03:44+00:00   \n",
       "...                            ...                       ...   \n",
       "15639798 2020-04-30 23:58:54+00:00 2020-04-30 23:58:54+00:00   \n",
       "15639799 2020-04-30 23:59:04+00:00 2020-04-30 23:59:04+00:00   \n",
       "15639800 2020-04-30 23:59:26+00:00 2020-04-30 23:59:26+00:00   \n",
       "15639801 2020-04-30 23:59:33+00:00 2020-04-30 23:59:37+00:00   \n",
       "15639802 2020-04-30 23:59:46+00:00 2020-04-30 23:59:46+00:00   \n",
       "\n",
       "         fav_main_category   fav_sub_category  \n",
       "0                  unknown            unknown  \n",
       "1                  apparel    shoes.moccasins  \n",
       "2                computers        bedroom.bed  \n",
       "3             construction  components.faucet  \n",
       "4                furniture        bedroom.bed  \n",
       "...                    ...                ...  \n",
       "15639798           apparel              shoes  \n",
       "15639799         furniture    universal.light  \n",
       "15639800           apparel              shoes  \n",
       "15639801      construction        tools.light  \n",
       "15639802           apparel              shoes  \n",
       "\n",
       "[15639803 rows x 13 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_all_user_feature_chunks_aggregated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c0cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c9a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acf6e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_features_from_chunk_main2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "    df['date'] = df['event_time'].dt.date\n",
    "    print(\"from event_type to purchase\")\n",
    "    purchase_df = df[df['event_type'] == 'purchase']\n",
    "    print(\"grouping by user_id\")\n",
    "    grouped = df.groupby('user_id')\n",
    "    print(\"starting the aggregation\")\n",
    "    features = grouped.agg(\n",
    "        count_view=('event_type', lambda x: (x == 'view').sum()),\n",
    "        count_cart=('event_type', lambda x: (x == 'cart').sum()),\n",
    "        count_purchase=('event_type', lambda x: (x == 'purchase').sum()),\n",
    "        unique_sessions=('user_session', pd.Series.nunique),\n",
    "        fav_main_category=('main_category', lambda x: x.mode(\n",
    "        ).iloc[0] if not x.mode().empty else 'unknown'),\n",
    "        fav_sub_category=('sub_category', lambda x: x.mode(\n",
    "        ).iloc[0] if not x.mode().empty else 'unknown'),\n",
    "        first_event=('event_time', 'min'),\n",
    "        last_event=('event_time', 'max'),\n",
    "        active_days=('date', 'nunique')\n",
    "    )\n",
    "    print(\"aggregation finished, starting total price and average\")\n",
    "    # Total & average purchase price\n",
    "    purchase_stats = purchase_df.groupby('user_id')['price'].agg(['sum', 'mean']).rename(\n",
    "        columns={'sum': 'total_spent', 'mean': 'avg_purchase_price'})\n",
    "\n",
    "    # Merge\n",
    "    print(\"merging\")\n",
    "    features = features.join(purchase_stats, how='left')\n",
    "    latest_date = features['last_event'].max()\n",
    "    features['recency_days'] = (latest_date - features['last_event']).dt.days\n",
    "    \n",
    "    features[[\"total_spent\", \"avg_purchase_price\"]] = features[[\n",
    "        \"total_spent\", \"avg_purchase_price\"]].fillna(0)\n",
    "    \n",
    "    features = features[\n",
    "        [\n",
    "            \"count_view\", \"count_cart\", \"count_purchase\",\n",
    "            \"unique_sessions\", \"fav_main_category\", \"fav_sub_category\",\n",
    "            \"active_days\", \"recency_days\",\n",
    "            \"total_spent\", \"avg_purchase_price\",\n",
    "            \"first_event\", \"last_event\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    return features.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ceae14aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_features_main2():\n",
    "    \"\"\"\n",
    "    Process ONLY the first .parquet file in data/interim to test feature extraction.\n",
    "    \"\"\"\n",
    "    os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "    files = sorted([f for f in os.listdir(\n",
    "        INTERIM_PATH) if f.endswith(\".parquet\")])\n",
    "    print(\"Building user features from chunks\")\n",
    "    for i, file in enumerate(tqdm(files, desc=\"Building user features from chunks\")):\n",
    "        chunk_path = os.path.join(INTERIM_PATH, file)\n",
    "        df = pd.read_parquet(chunk_path)\n",
    "\n",
    "        user_features = build_user_features_from_chunk_main2(df)\n",
    "        user_features.to_parquet(os.path.join(PROCESSED_PATH, f\"user_features_chunk_{i}.parquet\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_user_features_main():\n",
    "    \"\"\"\n",
    "    Merge all processed per-user parquet chunks into a single user_features.parquet file.\n",
    "    \"\"\"\n",
    "    files = sorted([f for f in os.listdir(PROCESSED_PATH)\n",
    "                   if f.startswith(\"user_features_chunk\")])\n",
    "    all_chunks = []\n",
    "\n",
    "    for file in tqdm(files, desc=\"Merging all user features\"):\n",
    "        chunk = pd.read_parquet(os.path.join(PROCESSED_PATH, file))\n",
    "        all_chunks.append(chunk)\n",
    "\n",
    "    df_all = pd.concat(all_chunks).groupby(\"user_id\").agg({\n",
    "        **{col: 'sum' for col in [\"count_view\", \"count_cart\", \"count_remove_from_cart\", \"count_purchase\", \"total_spent\", \"unique_sessions\", \"active_days\"]},\n",
    "        \"fav_sub_category\": lambda x: x.mode().iloc[0] if not x.mode().empty else \"unknown\"\n",
    "    }).reset_index()\n",
    "\n",
    "    df_all.to_parquet(os.path.join(\n",
    "        PROCESSED_PATH, \"user_features.parquet\"), index=False)\n",
    "    print(\"Final user_features.parquet saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc87029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c926c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaceb10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ad371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_features_from_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate user-level features from a chunk of cleaned data.\n",
    "    \"\"\"\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "\n",
    "    # Count events per type\n",
    "    event_counts = df.pivot_table(index='user_id',\n",
    "                                  columns='event_type',\n",
    "                                  aggfunc='size',\n",
    "                                  fill_value=0)\n",
    "\n",
    "    event_counts.columns = [f\"count_{col}\" for col in event_counts.columns]\n",
    "\n",
    "    # Total amount spent per user\n",
    "    df_purchase = df[df['event_type'] == 'purchase']\n",
    "    total_spent = df_purchase.groupby(\n",
    "        'user_id')['price'].sum().rename(\"total_spent\")\n",
    "\n",
    "    # Number of sessions per user\n",
    "    session_count = df.groupby(\n",
    "        'user_id')['user_session'].nunique().rename(\"unique_sessions\")\n",
    "\n",
    "    # Most frequent sub-category\n",
    "    top_sub_category = df.groupby('user_id')['sub_category'].agg(lambda x: x.mode(\n",
    "    ).iloc[0] if not x.mode().empty else \"unknown\").rename(\"fav_sub_category\")\n",
    "\n",
    "    # First and last event time per user\n",
    "    first_event = df.groupby('user_id')[\n",
    "        'event_time'].min().rename(\"first_event\")\n",
    "    last_event = df.groupby('user_id')['event_time'].max().rename(\"last_event\")\n",
    "\n",
    "    # Combine all features\n",
    "    features = pd.concat([event_counts, total_spent, session_count,\n",
    "                         top_sub_category, first_event, last_event], axis=1).reset_index()\n",
    "\n",
    "    # Add recency and activity length\n",
    "    features[\"active_days\"] = (\n",
    "        features[\"last_event\"] - features[\"first_event\"]).dt.days + 1\n",
    "    \n",
    "    features[[\"total_spent\", \"avg_purchase_price\"]] = features[[\n",
    "        \"total_spent\", \"avg_purchase_price\"]].fillna(0)\n",
    "\n",
    "    return features.drop(columns=[\"first_event\", \"last_event\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618daadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_features_main():\n",
    "    \"\"\"\n",
    "    Process ONLY the first .parquet file in data/interim to test feature extraction.\n",
    "    \"\"\"\n",
    "    os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "    files = sorted([f for f in os.listdir(\n",
    "        INTERIM_PATH) if f.endswith(\".parquet\")])\n",
    "\n",
    "    # Test only on the first file\n",
    "    test_file = files[2]\n",
    "    print(f\"🧪 Testing with file: {test_file}\")\n",
    "    chunk_path = os.path.join(INTERIM_PATH, test_file)\n",
    "    df = pd.read_parquet(chunk_path)\n",
    "    print(\"Start building fatures\")\n",
    "    user_features = build_user_features_from_chunk_main(df)\n",
    "    print(\"Features builded and saving new file\")\n",
    "    user_features.to_parquet(os.path.join(\n",
    "        PROCESSED_PATH, \"user_features_chunk_test.parquet\"), index=False)\n",
    "    print(\"Test features saved as user_features_chunk_test2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86f4ff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing with file: cleaned_chunk_1.parquet\n",
      "Start building fatures\n",
      "Features builded and saving new file\n",
      "Test features saved as user_features_chunk_test1.parquet\n"
     ]
    }
   ],
   "source": [
    "build_all_features_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cbc74d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing with file: cleaned_chunk_10.parquet\n",
      "Start building fatures\n",
      "from event_type to purchase\n",
      "grouping by user_id\n",
      "starting the aggregation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_40804\\1351636381.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m build_all_features_test()\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_40804\\129003010.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m     print(f\"🧪 Testing with file: {test_file}\")\n\u001b[32m     12\u001b[39m     chunk_path = os.path.join(INTERIM_PATH, test_file)\n\u001b[32m     13\u001b[39m     df = pd.read_parquet(chunk_path)\n\u001b[32m     14\u001b[39m     print(\u001b[33m\"Start building fatures\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     user_features = build_user_features_from_chunk_test(df)\n\u001b[32m     16\u001b[39m     print(\u001b[33m\"Features builded and saving new file\"\u001b[39m)\n\u001b[32m     17\u001b[39m     user_features.to_parquet(os.path.join(\n\u001b[32m     18\u001b[39m         PROCESSED_PATH, \u001b[33m\"user_features_chunk_test.parquet\"\u001b[39m), index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_40804\\3606465721.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m      5\u001b[39m     purchase_df = df[df[\u001b[33m'event_type'\u001b[39m] == \u001b[33m'purchase'\u001b[39m]\n\u001b[32m      6\u001b[39m     print(\u001b[33m\"grouping by user_id\"\u001b[39m)\n\u001b[32m      7\u001b[39m     grouped = df.groupby(\u001b[33m'user_id'\u001b[39m)\n\u001b[32m      8\u001b[39m     print(\u001b[33m\"starting the aggregation\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     features = grouped.agg(\n\u001b[32m     10\u001b[39m         count_view=(\u001b[33m'event_type'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m x: (x == \u001b[33m'view'\u001b[39m).sum()),\n\u001b[32m     11\u001b[39m         count_cart=(\u001b[33m'event_type'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m x: (x == \u001b[33m'cart'\u001b[39m).sum()),\n\u001b[32m     12\u001b[39m         count_purchase=(\u001b[33m'event_type'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m x: (x == \u001b[33m'purchase'\u001b[39m).sum()),\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   1428\u001b[39m             kwargs[\u001b[33m\"engine\"\u001b[39m] = engine\n\u001b[32m   1429\u001b[39m             kwargs[\u001b[33m\"engine_kwargs\"\u001b[39m] = engine_kwargs\n\u001b[32m   1430\u001b[39m \n\u001b[32m   1431\u001b[39m         op = GroupByApply(self, func, args=args, kwargs=kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m         result = op.agg()\n\u001b[32m   1433\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;28;01mand\u001b[39;00m result \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1434\u001b[39m             \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[32m   1435\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.as_index \u001b[38;5;28;01mand\u001b[39;00m is_list_like(func):\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(func, str):\n\u001b[32m    187\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.apply_str()\n\u001b[32m    188\u001b[39m \n\u001b[32m    189\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.agg_dict_like()\n\u001b[32m    191\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[32m    192\u001b[39m             \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[32m    193\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.agg_list_like()\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    419\u001b[39m         Returns\n\u001b[32m    420\u001b[39m         -------\n\u001b[32m    421\u001b[39m         Result of aggregation.\n\u001b[32m    422\u001b[39m         \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.agg_or_apply_dict_like(op_name=\u001b[33m\"agg\"\u001b[39m)\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, op_name)\u001b[39m\n\u001b[32m   1604\u001b[39m \n\u001b[32m   1605\u001b[39m         with com.temp_setattr(\n\u001b[32m   1606\u001b[39m             obj, \u001b[33m\"as_index\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition=hasattr(obj, \u001b[33m\"as_index\"\u001b[39m)\n\u001b[32m   1607\u001b[39m         ):\n\u001b[32m-> \u001b[39m\u001b[32m1608\u001b[39m             result_index, result_data = self.compute_dict_like(\n\u001b[32m   1609\u001b[39m                 op_name, selected_obj, selection, kwargs\n\u001b[32m   1610\u001b[39m             )\n\u001b[32m   1611\u001b[39m         result = self.wrap_results_dict_like(selected_obj, result_index, result_data)\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, op_name, selected_obj, selection, kwargs)\u001b[39m\n\u001b[32m    492\u001b[39m                 keys += [key] * len(key_data)\n\u001b[32m    493\u001b[39m                 results += key_data\n\u001b[32m    494\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    495\u001b[39m             \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m             results = [\n\u001b[32m    497\u001b[39m                 getattr(obj._gotitem(key, ndim=\u001b[32m1\u001b[39m), op_name)(how, **kwargs)\n\u001b[32m    498\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;28;01min\u001b[39;00m func.items()\n\u001b[32m    499\u001b[39m             ]\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    253\u001b[39m             \u001b[38;5;66;03m# but not the class list / tuple itself.\u001b[39;00m\n\u001b[32m    254\u001b[39m             func = maybe_mangle_lambdas(func)\n\u001b[32m    255\u001b[39m             kwargs[\u001b[33m\"engine\"\u001b[39m] = engine\n\u001b[32m    256\u001b[39m             kwargs[\u001b[33m\"engine_kwargs\"\u001b[39m] = engine_kwargs\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m             ret = self._aggregate_multiple_funcs(func, *args, **kwargs)\n\u001b[32m    258\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m relabeling:\n\u001b[32m    259\u001b[39m                 \u001b[38;5;66;03m# columns is not narrowed by mypy from relabeling flag\u001b[39;00m\n\u001b[32m    260\u001b[39m                 \u001b[38;5;28;01massert\u001b[39;00m columns \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, arg, *args, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m             \u001b[38;5;66;03m# Combine results using the index, need to adjust index after\u001b[39;00m\n\u001b[32m    359\u001b[39m             \u001b[38;5;66;03m# if as_index=False (GH#50724)\u001b[39;00m\n\u001b[32m    360\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, func) \u001b[38;5;28;01min\u001b[39;00m enumerate(arg):\n\u001b[32m    361\u001b[39m                 key = base.OutputKey(label=name, position=idx)\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m                 results[key] = self.aggregate(func, *args, **kwargs)\n\u001b[32m    363\u001b[39m \n\u001b[32m    364\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m any(isinstance(x, DataFrame) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m results.values()):\n\u001b[32m    365\u001b[39m             \u001b[38;5;28;01mfrom\u001b[39;00m pandas \u001b[38;5;28;01mimport\u001b[39;00m concat\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m self._python_agg_general(func, *args, **kwargs)\n\u001b[32m    292\u001b[39m \n\u001b[32m    293\u001b[39m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    294\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m self._python_agg_general(func, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m KeyError:\n\u001b[32m    296\u001b[39m                 \u001b[38;5;66;03m# KeyError raised in test_groupby.test_basic is bc the func does\u001b[39;00m\n\u001b[32m    297\u001b[39m                 \u001b[38;5;66;03m#  a dictionary lookup on group.name, but group name is not\u001b[39;00m\n\u001b[32m    298\u001b[39m                 \u001b[38;5;66;03m#  pinned in _python_agg_general, only in _aggregate_named\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m             warn_alias_replacement(self, orig_func, alias)\n\u001b[32m    324\u001b[39m         f = \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, *args, **kwargs)\n\u001b[32m    325\u001b[39m \n\u001b[32m    326\u001b[39m         obj = self._obj_with_exclusions\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         result = self._grouper.agg_series(obj, f)\n\u001b[32m    328\u001b[39m         res = obj._constructor(result, name=obj.name)\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._wrap_aggregated_output(res)\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, obj, func, preserve_dtype)\u001b[39m\n\u001b[32m    860\u001b[39m             \u001b[38;5;66;03m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[32m    861\u001b[39m             \u001b[38;5;66;03m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[32m    862\u001b[39m             preserve_dtype = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m         result = self._aggregate_series_pure_python(obj, func)\n\u001b[32m    865\u001b[39m \n\u001b[32m    866\u001b[39m         npvalues = lib.maybe_convert_objects(result, try_float=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    867\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m preserve_dtype:\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, obj, func)\u001b[39m\n\u001b[32m    881\u001b[39m \n\u001b[32m    882\u001b[39m         splitter = self._get_splitter(obj, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    883\u001b[39m \n\u001b[32m    884\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;28;01min\u001b[39;00m enumerate(splitter):\n\u001b[32m--> \u001b[39m\u001b[32m885\u001b[39m             res = func(group)\n\u001b[32m    886\u001b[39m             res = extract_result(res)\n\u001b[32m    887\u001b[39m \n\u001b[32m    888\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m initialized:\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m         f = \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, *args, **kwargs)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_40804\\3606465721.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         count_cart=(\u001b[33m'event_type'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m x: (x == \u001b[33m'cart'\u001b[39m).sum()),\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m                     \u001b[38;5;28;01mreturn\u001b[39;00m NotImplemented\n\u001b[32m     73\u001b[39m \n\u001b[32m     74\u001b[39m         other = item_from_zerodim(other)\n\u001b[32m     75\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m method(self, other)\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\arraylike.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     38\u001b[39m     @unpack_zerodim_and_defer(\u001b[33m\"__eq__\"\u001b[39m)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m __eq__(self, other):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._cmp_method(other, operator.eq)\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\series.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6117\u001b[39m         rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   6118\u001b[39m \n\u001b[32m   6119\u001b[39m         res_values = ops.comparison_op(lvalues, rvalues, op)\n\u001b[32m   6120\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m6121\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._construct_result(res_values, name=res_name)\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\series.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, result, name)\u001b[39m\n\u001b[32m   6228\u001b[39m         \u001b[38;5;66;03m# TODO: result should always be ArrayLike, but this fails for some\u001b[39;00m\n\u001b[32m   6229\u001b[39m         \u001b[38;5;66;03m#  JSONArray tests\u001b[39;00m\n\u001b[32m   6230\u001b[39m         dtype = getattr(result, \u001b[33m\"dtype\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   6231\u001b[39m         out = self._constructor(result, index=self.index, dtype=dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6232\u001b[39m         out = out.__finalize__(self)\n\u001b[32m   6233\u001b[39m \n\u001b[32m   6234\u001b[39m         \u001b[38;5;66;03m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[32m   6235\u001b[39m         \u001b[38;5;66;03m#  would set it back to self.name\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\Test\\Documents\\Projects\\5BIS\\amazing-client-clustering\\.env\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, other, method, **kwargs)\u001b[39m\n\u001b[32m   6260\u001b[39m                 self.attrs = deepcopy(other.attrs)\n\u001b[32m   6261\u001b[39m \n\u001b[32m   6262\u001b[39m             self.flags.allows_duplicate_labels = other.flags.allows_duplicate_labels\n\u001b[32m   6263\u001b[39m             \u001b[38;5;66;03m# For subclasses using _metadata.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m set(self._metadata) & set(other._metadata):\n\u001b[32m   6265\u001b[39m                 \u001b[38;5;28;01massert\u001b[39;00m isinstance(name, str)\n\u001b[32m   6266\u001b[39m                 object.__setattr__(self, name, getattr(other, name, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m   6267\u001b[39m \n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "build_all_features_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745542ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_features():\n",
    "    \"\"\"\n",
    "    Process ONLY the first .parquet file in data/interim to test feature extraction.\n",
    "    \"\"\"\n",
    "    os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "    files = sorted([f for f in os.listdir(\n",
    "        INTERIM_PATH) if f.endswith(\".parquet\")])\n",
    "\n",
    "    # Test only on the first file\n",
    "    test_file = files[1]\n",
    "    print(f\"🧪 Testing with file: {test_file}\")\n",
    "    chunk_path = os.path.join(INTERIM_PATH, test_file)\n",
    "    df = pd.read_parquet(chunk_path)\n",
    "    print(\"Start building fatures\")\n",
    "    user_features = build_user_features_from_chunk(df)\n",
    "    print(\"Features builded and saving new file\")\n",
    "    user_features.to_parquet(os.path.join(\n",
    "        PROCESSED_PATH, \"user_features_chunk_test.parquet\"), index=False)\n",
    "    print(\"Test features saved as user_features_chunk_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4930504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chuncked = pd.read_parquet(os.path.join(INTERIM_PATH, \"cleaned_chunk_0.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31d6eaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>event_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_session</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-01 00:00:00+00:00</td>\n",
       "      <td>view</td>\n",
       "      <td>apple</td>\n",
       "      <td>1302.48</td>\n",
       "      <td>556695836</td>\n",
       "      <td>ca5eefc5-11f9-450c-91ed-380285a0bc80</td>\n",
       "      <td>construction</td>\n",
       "      <td>tools.light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-12-01 00:00:00+00:00</td>\n",
       "      <td>view</td>\n",
       "      <td>force</td>\n",
       "      <td>102.96</td>\n",
       "      <td>577702456</td>\n",
       "      <td>de33debe-c7bf-44e8-8a12-3bf8421f842a</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-12-01 00:00:01+00:00</td>\n",
       "      <td>view</td>\n",
       "      <td>bosch</td>\n",
       "      <td>313.52</td>\n",
       "      <td>539453785</td>\n",
       "      <td>5ee185a7-0689-4a33-923d-ba0130929a76</td>\n",
       "      <td>appliances</td>\n",
       "      <td>personal.massager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-01 00:00:02+00:00</td>\n",
       "      <td>purchase</td>\n",
       "      <td>unknown</td>\n",
       "      <td>132.31</td>\n",
       "      <td>535135317</td>\n",
       "      <td>61792a26-672f-4e61-9832-7b63bb1714db</td>\n",
       "      <td>computers</td>\n",
       "      <td>peripherals.printer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-01 00:00:02+00:00</td>\n",
       "      <td>view</td>\n",
       "      <td>nika</td>\n",
       "      <td>101.68</td>\n",
       "      <td>517987650</td>\n",
       "      <td>906c6ca8-ff5c-419a-bde9-967ba8e2233e</td>\n",
       "      <td>apparel</td>\n",
       "      <td>trousers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-12-01 00:00:02+00:00</td>\n",
       "      <td>view</td>\n",
       "      <td>ikea</td>\n",
       "      <td>163.56</td>\n",
       "      <td>542860793</td>\n",
       "      <td>a1bcb550-1065-4769-a80a-0ccb4bcee78d</td>\n",
       "      <td>accessories</td>\n",
       "      <td>umbrella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-12-01 00:00:02+00:00</td>\n",
       "      <td>view</td>\n",
       "      <td>unknown</td>\n",
       "      <td>88.81</td>\n",
       "      <td>538021416</td>\n",
       "      <td>e88f77cc-e75e-4e9f-9ef6-ef1a302ed50a</td>\n",
       "      <td>electronics</td>\n",
       "      <td>clocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-12-01 00:00:03+00:00</td>\n",
       "      <td>view</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>256.38</td>\n",
       "      <td>525740700</td>\n",
       "      <td>370e8c88-3d07-41df-9aaa-2adf5a0bf312</td>\n",
       "      <td>construction</td>\n",
       "      <td>tools.light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-12-01 00:00:04+00:00</td>\n",
       "      <td>view</td>\n",
       "      <td>jet</td>\n",
       "      <td>20.57</td>\n",
       "      <td>512509221</td>\n",
       "      <td>4227259f-1c4c-41dc-84b5-9354d864eefa</td>\n",
       "      <td>computers</td>\n",
       "      <td>notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-12-01 00:00:04+00:00</td>\n",
       "      <td>view</td>\n",
       "      <td>unknown</td>\n",
       "      <td>179.16</td>\n",
       "      <td>553345124</td>\n",
       "      <td>58c692ff-c7a9-4e35-9ec4-58598f1940e0</td>\n",
       "      <td>construction</td>\n",
       "      <td>components.faucet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 event_time event_type    brand    price    user_id  \\\n",
       "0 2019-12-01 00:00:00+00:00       view    apple  1302.48  556695836   \n",
       "1 2019-12-01 00:00:00+00:00       view    force   102.96  577702456   \n",
       "2 2019-12-01 00:00:01+00:00       view    bosch   313.52  539453785   \n",
       "3 2019-12-01 00:00:02+00:00   purchase  unknown   132.31  535135317   \n",
       "4 2019-12-01 00:00:02+00:00       view     nika   101.68  517987650   \n",
       "5 2019-12-01 00:00:02+00:00       view     ikea   163.56  542860793   \n",
       "6 2019-12-01 00:00:02+00:00       view  unknown    88.81  538021416   \n",
       "7 2019-12-01 00:00:03+00:00       view   xiaomi   256.38  525740700   \n",
       "8 2019-12-01 00:00:04+00:00       view      jet    20.57  512509221   \n",
       "9 2019-12-01 00:00:04+00:00       view  unknown   179.16  553345124   \n",
       "\n",
       "                           user_session main_category         sub_category  \n",
       "0  ca5eefc5-11f9-450c-91ed-380285a0bc80  construction          tools.light  \n",
       "1  de33debe-c7bf-44e8-8a12-3bf8421f842a       unknown              unknown  \n",
       "2  5ee185a7-0689-4a33-923d-ba0130929a76    appliances    personal.massager  \n",
       "3  61792a26-672f-4e61-9832-7b63bb1714db     computers  peripherals.printer  \n",
       "4  906c6ca8-ff5c-419a-bde9-967ba8e2233e       apparel             trousers  \n",
       "5  a1bcb550-1065-4769-a80a-0ccb4bcee78d   accessories             umbrella  \n",
       "6  e88f77cc-e75e-4e9f-9ef6-ef1a302ed50a   electronics               clocks  \n",
       "7  370e8c88-3d07-41df-9aaa-2adf5a0bf312  construction          tools.light  \n",
       "8  4227259f-1c4c-41dc-84b5-9354d864eefa     computers             notebook  \n",
       "9  58c692ff-c7a9-4e35-9ec4-58598f1940e0  construction    components.faucet  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chuncked.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4b40e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000000 entries, 0 to 29999999\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Dtype              \n",
      "---  ------         -----              \n",
      " 0   event_time     datetime64[ns, UTC]\n",
      " 1   event_type     object             \n",
      " 2   brand          object             \n",
      " 3   price          float64            \n",
      " 4   user_id        int64              \n",
      " 5   user_session   object             \n",
      " 6   main_category  object             \n",
      " 7   sub_category   object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), int64(1), object(5)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "chuncked.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62dd45fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing with file: cleaned_chunk_0.parquet\n",
      "Start building fatures\n",
      "Features builded and saving new file\n",
      "Test features saved as user_features_chunk_test.parquet\n"
     ]
    }
   ],
   "source": [
    "build_all_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_1 = pd.read_parquet(os.path.join(\n",
    "    PROCESSED_PATH, \"user_features_chunk_test.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8d12554",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_2 = pd.read_parquet(os.path.join(\n",
    "    PROCESSED_PATH, \"user_features_chunk_test.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cb72ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2624800 entries, 0 to 2624799\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype              \n",
      "---  ------              -----              \n",
      " 0   user_id             int64              \n",
      " 1   count_view          int64              \n",
      " 2   count_cart          int64              \n",
      " 3   count_purchase      int64              \n",
      " 4   unique_sessions     int64              \n",
      " 5   fav_main_category   object             \n",
      " 6   fav_sub_category    object             \n",
      " 7   first_event         datetime64[ns, UTC]\n",
      " 8   last_event          datetime64[ns, UTC]\n",
      " 9   active_days         int64              \n",
      " 10  total_spent         float64            \n",
      " 11  avg_purchase_price  float64            \n",
      " 12  recency_days        int64              \n",
      "dtypes: datetime64[ns, UTC](2), float64(2), int64(7), object(2)\n",
      "memory usage: 260.3+ MB\n"
     ]
    }
   ],
   "source": [
    "test_features_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "352d5a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>count_view</th>\n",
       "      <th>count_cart</th>\n",
       "      <th>count_purchase</th>\n",
       "      <th>unique_sessions</th>\n",
       "      <th>fav_main_category</th>\n",
       "      <th>fav_sub_category</th>\n",
       "      <th>first_event</th>\n",
       "      <th>last_event</th>\n",
       "      <th>active_days</th>\n",
       "      <th>total_spent</th>\n",
       "      <th>avg_purchase_price</th>\n",
       "      <th>recency_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30493659</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>construction</td>\n",
       "      <td>tools.light</td>\n",
       "      <td>2019-12-22 18:49:44+00:00</td>\n",
       "      <td>2019-12-23 04:50:04+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32836036</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2019-12-26 02:49:31+00:00</td>\n",
       "      <td>2019-12-26 02:49:31+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39480587</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>appliances</td>\n",
       "      <td>kitchen.hob</td>\n",
       "      <td>2019-12-22 16:56:26+00:00</td>\n",
       "      <td>2019-12-24 19:39:22+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40484041</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>appliances</td>\n",
       "      <td>kitchen.dishwasher</td>\n",
       "      <td>2019-12-18 09:21:46+00:00</td>\n",
       "      <td>2019-12-18 09:22:26+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49484535</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>accessories</td>\n",
       "      <td>wallet</td>\n",
       "      <td>2019-12-20 18:30:18+00:00</td>\n",
       "      <td>2019-12-23 05:49:37+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58438489</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>apparel</td>\n",
       "      <td>shorts</td>\n",
       "      <td>2019-12-28 05:42:00+00:00</td>\n",
       "      <td>2019-12-28 05:42:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62336140</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2019-12-22 16:19:22+00:00</td>\n",
       "      <td>2019-12-22 16:19:22+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63518127</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>apparel</td>\n",
       "      <td>bicycle</td>\n",
       "      <td>2019-12-24 05:32:38+00:00</td>\n",
       "      <td>2019-12-24 07:38:27+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68576588</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>electronics</td>\n",
       "      <td>clocks</td>\n",
       "      <td>2019-12-27 14:37:59+00:00</td>\n",
       "      <td>2019-12-27 14:37:59+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>70829073</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2019-12-20 16:45:02+00:00</td>\n",
       "      <td>2019-12-22 16:35:09+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  count_view  count_cart  count_purchase  unique_sessions  \\\n",
       "0  30493659           2           0               0                2   \n",
       "1  32836036           1           0               0                1   \n",
       "2  39480587          12           1               0                3   \n",
       "3  40484041           3           0               0                1   \n",
       "4  49484535           3           0               0                3   \n",
       "5  58438489           1           0               0                1   \n",
       "6  62336140           1           0               0                1   \n",
       "7  63518127          17           0               0                2   \n",
       "8  68576588           1           0               0                1   \n",
       "9  70829073           5           0               0                3   \n",
       "\n",
       "  fav_main_category    fav_sub_category               first_event  \\\n",
       "0      construction         tools.light 2019-12-22 18:49:44+00:00   \n",
       "1           unknown             unknown 2019-12-26 02:49:31+00:00   \n",
       "2        appliances         kitchen.hob 2019-12-22 16:56:26+00:00   \n",
       "3        appliances  kitchen.dishwasher 2019-12-18 09:21:46+00:00   \n",
       "4       accessories              wallet 2019-12-20 18:30:18+00:00   \n",
       "5           apparel              shorts 2019-12-28 05:42:00+00:00   \n",
       "6           unknown             unknown 2019-12-22 16:19:22+00:00   \n",
       "7           apparel             bicycle 2019-12-24 05:32:38+00:00   \n",
       "8       electronics              clocks 2019-12-27 14:37:59+00:00   \n",
       "9           unknown             unknown 2019-12-20 16:45:02+00:00   \n",
       "\n",
       "                 last_event  active_days  total_spent  avg_purchase_price  \\\n",
       "0 2019-12-23 04:50:04+00:00            2          NaN                 NaN   \n",
       "1 2019-12-26 02:49:31+00:00            1          NaN                 NaN   \n",
       "2 2019-12-24 19:39:22+00:00            3          NaN                 NaN   \n",
       "3 2019-12-18 09:22:26+00:00            1          NaN                 NaN   \n",
       "4 2019-12-23 05:49:37+00:00            3          NaN                 NaN   \n",
       "5 2019-12-28 05:42:00+00:00            1          NaN                 NaN   \n",
       "6 2019-12-22 16:19:22+00:00            1          NaN                 NaN   \n",
       "7 2019-12-24 07:38:27+00:00            1          NaN                 NaN   \n",
       "8 2019-12-27 14:37:59+00:00            1          NaN                 NaN   \n",
       "9 2019-12-22 16:35:09+00:00            2          NaN                 NaN   \n",
       "\n",
       "   recency_days  \n",
       "0             5  \n",
       "1             2  \n",
       "2             3  \n",
       "3            10  \n",
       "4             5  \n",
       "5             0  \n",
       "6             5  \n",
       "7             4  \n",
       "8             1  \n",
       "9             5  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c94f2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2513903 entries, 0 to 2513902\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   user_id           int64  \n",
      " 1   count_cart        int64  \n",
      " 2   count_purchase    int64  \n",
      " 3   count_view        int64  \n",
      " 4   total_spent       float64\n",
      " 5   unique_sessions   int64  \n",
      " 6   fav_sub_category  object \n",
      " 7   active_days       int64  \n",
      "dtypes: float64(1), int64(6), object(1)\n",
      "memory usage: 153.4+ MB\n"
     ]
    }
   ],
   "source": [
    "test_features_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f176f6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>count_cart</th>\n",
       "      <th>count_purchase</th>\n",
       "      <th>count_view</th>\n",
       "      <th>total_spent</th>\n",
       "      <th>unique_sessions</th>\n",
       "      <th>active_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.513903e+06</td>\n",
       "      <td>2.513903e+06</td>\n",
       "      <td>2.513903e+06</td>\n",
       "      <td>2.513903e+06</td>\n",
       "      <td>233501.000000</td>\n",
       "      <td>2.513903e+06</td>\n",
       "      <td>2.513903e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.533412e+08</td>\n",
       "      <td>5.614318e-01</td>\n",
       "      <td>1.873227e-01</td>\n",
       "      <td>1.118488e+01</td>\n",
       "      <td>604.777046</td>\n",
       "      <td>2.741512e+00</td>\n",
       "      <td>3.272886e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.834613e+07</td>\n",
       "      <td>2.381645e+00</td>\n",
       "      <td>1.200075e+00</td>\n",
       "      <td>3.211536e+01</td>\n",
       "      <td>1768.718047</td>\n",
       "      <td>1.951251e+01</td>\n",
       "      <td>4.041068e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.951588e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.254652e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>102.940000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.573829e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>230.640000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.813455e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>591.630000</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.880848e+08</td>\n",
       "      <td>3.910000e+02</td>\n",
       "      <td>3.150000e+02</td>\n",
       "      <td>2.338800e+04</td>\n",
       "      <td>236749.030000</td>\n",
       "      <td>2.293800e+04</td>\n",
       "      <td>1.700000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id    count_cart  count_purchase    count_view  \\\n",
       "count  2.513903e+06  2.513903e+06    2.513903e+06  2.513903e+06   \n",
       "mean   5.533412e+08  5.614318e-01    1.873227e-01  1.118488e+01   \n",
       "std    2.834613e+07  2.381645e+00    1.200075e+00  3.211536e+01   \n",
       "min    2.951588e+07  0.000000e+00    0.000000e+00  0.000000e+00   \n",
       "25%    5.254652e+08  0.000000e+00    0.000000e+00  1.000000e+00   \n",
       "50%    5.573829e+08  0.000000e+00    0.000000e+00  4.000000e+00   \n",
       "75%    5.813455e+08  0.000000e+00    0.000000e+00  1.100000e+01   \n",
       "max    5.880848e+08  3.910000e+02    3.150000e+02  2.338800e+04   \n",
       "\n",
       "         total_spent  unique_sessions   active_days  \n",
       "count  233501.000000     2.513903e+06  2.513903e+06  \n",
       "mean      604.777046     2.741512e+00  3.272886e+00  \n",
       "std      1768.718047     1.951251e+01  4.041068e+00  \n",
       "min         0.850000     1.000000e+00  1.000000e+00  \n",
       "25%       102.940000     1.000000e+00  1.000000e+00  \n",
       "50%       230.640000     1.000000e+00  1.000000e+00  \n",
       "75%       591.630000     3.000000e+00  4.000000e+00  \n",
       "max    236749.030000     2.293800e+04  1.700000e+01  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9fa680e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>count_cart</th>\n",
       "      <th>count_purchase</th>\n",
       "      <th>count_view</th>\n",
       "      <th>total_spent</th>\n",
       "      <th>unique_sessions</th>\n",
       "      <th>fav_sub_category</th>\n",
       "      <th>active_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29515875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>bedroom.bed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31198833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>tools.light</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34916060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>video.projector</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38661019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>shoes.sandals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42896738</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>tools.light</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49484535</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>unknown</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56931866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>kitchen.refrigerators</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>62336140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>shoes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>63518127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>kitchen.chair</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>65746813</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>audio.headphone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  count_cart  count_purchase  count_view  total_spent  \\\n",
       "0  29515875           0               0           1          NaN   \n",
       "1  31198833           0               0           3          NaN   \n",
       "2  34916060           0               0           1          NaN   \n",
       "3  38661019           0               0           2          NaN   \n",
       "4  42896738           0               0           1          NaN   \n",
       "5  49484535           0               0           2          NaN   \n",
       "6  56931866           0               0           5          NaN   \n",
       "7  62336140           0               0           1          NaN   \n",
       "8  63518127           0               0           1          NaN   \n",
       "9  65746813           0               0          10          NaN   \n",
       "\n",
       "   unique_sessions       fav_sub_category  active_days  \n",
       "0                1            bedroom.bed            1  \n",
       "1                1            tools.light            1  \n",
       "2                1        video.projector            1  \n",
       "3                1          shoes.sandals            1  \n",
       "4                1            tools.light            1  \n",
       "5                2                unknown            8  \n",
       "6                2  kitchen.refrigerators            8  \n",
       "7                1                  shoes            1  \n",
       "8                1          kitchen.chair            1  \n",
       "9                9        audio.headphone            1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7837ebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                     0\n",
      "count_view                  0\n",
      "count_cart                  0\n",
      "count_purchase              0\n",
      "unique_sessions             0\n",
      "fav_main_category           0\n",
      "fav_sub_category            0\n",
      "first_event                 0\n",
      "last_event                  0\n",
      "active_days                 0\n",
      "total_spent           2354202\n",
      "avg_purchase_price    2354202\n",
      "recency_days                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_features_2.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc987bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_features():\n",
    "    \"\"\"\n",
    "    Process all .parquet files in data/interim and save user features in data/processed.\n",
    "    \"\"\"\n",
    "    os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "    files = sorted([f for f in os.listdir(\n",
    "        INTERIM_PATH) if f.endswith(\".parquet\")])\n",
    "\n",
    "    for i, file in enumerate(tqdm(files, desc=\"Building user features from chunks\")):\n",
    "        chunk_path = os.path.join(INTERIM_PATH, file)\n",
    "        df = pd.read_parquet(chunk_path)\n",
    "\n",
    "        user_features = build_user_features_from_chunk(df)\n",
    "        user_features.to_parquet(os.path.join(\n",
    "            PROCESSED_PATH, f\"user_features_chunk_{i}.parquet\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e406620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_user_features():\n",
    "    \"\"\"\n",
    "    Merge all processed per-user parquet chunks into a single user_features.parquet file.\n",
    "    \"\"\"\n",
    "    files = sorted([f for f in os.listdir(PROCESSED_PATH)\n",
    "                   if f.startswith(\"user_features_chunk\")])\n",
    "    all_chunks = []\n",
    "\n",
    "    for file in tqdm(files, desc=\"Merging all user features\"):\n",
    "        chunk = pd.read_parquet(os.path.join(PROCESSED_PATH, file))\n",
    "        all_chunks.append(chunk)\n",
    "\n",
    "    df_all = pd.concat(all_chunks).groupby(\"user_id\").agg({\n",
    "        **{col: 'sum' for col in [\"count_view\", \"count_cart\", \"count_remove_from_cart\", \"count_purchase\", \"total_spent\", \"unique_sessions\", \"active_days\"]},\n",
    "        \"fav_sub_category\": lambda x: x.mode().iloc[0] if not x.mode().empty else \"unknown\"\n",
    "    }).reset_index()\n",
    "\n",
    "    df_all.to_parquet(os.path.join(\n",
    "        PROCESSED_PATH, \"user_features.parquet\"), index=False)\n",
    "    print(\"Final user_features.parquet saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c31c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    build_all_features()\n",
    "    merge_all_user_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
