{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f647f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import logging\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def load_csv_in_chunks(file_path: str, chunk_size):\n",
    "    \"\"\"Docstring\"\"\"\n",
    "    logging.info(\"Reading file in chunks: %s\", file_path)\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, sep=\",\", chunksize=chunk_size):\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "def load_all_months(data_folder: str, chunck_size):\n",
    "    \"\"\"Docstring\"\"\"\n",
    "    files = sorted(glob(os.path.join(data_folder, \"*.csv\")))\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        logging.info(\"Starting file: %s\", file)\n",
    "        yield from load_csv_in_chunks(file_path, chunck_size)\n",
    "\n",
    "\n",
    "def save_chunk_to_parquet(df, chunk_index, output_folder):\n",
    "    \"\"\"\n",
    "    Save a cleaned DataFrame chunk to a Parquet file with a consistent naming scheme.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The cleaned DataFrame to save.\n",
    "        chunk_index (int): The index of the current chunk.\n",
    "        output_folder (str): Destination folder where to save the Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the saved Parquet file.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    output_path = os.path.join(\n",
    "        output_folder, f\"cleaned_chunk_{chunk_index}.parquet\")\n",
    "\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    logging.info(\"Saved cleaned_chunk_%s to %s\", chunk_index, output_path)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def load_all_parquet(parquet_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and concatenates all .parquet files in the specified folder.\n",
    "\n",
    "    Args:\n",
    "        parquet_folder (str): Path to the folder containing .parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame with all the loaded data.\n",
    "    \"\"\"\n",
    "    parquet_files = sorted(glob(os.path.join(parquet_folder, \"*.parquet\")))\n",
    "\n",
    "    if not parquet_files:\n",
    "        logging.warning(\"No .parquet files found in %s\", parquet_folder)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    logging.info(\"Found %d .parquet files in %s\",\n",
    "                 len(parquet_files), parquet_folder)\n",
    "\n",
    "    dfs = []\n",
    "    for path in parquet_files:\n",
    "        logging.info(\"Loading: %s\", path)\n",
    "        dfs.append(pd.read_parquet(path))\n",
    "\n",
    "    full_df = pd.concat(dfs, ignore_index=True)\n",
    "    logging.info(\"All files loaded. Final shape: %s\", full_df.shape)\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2111cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def clean_chunk(df: pd.DataFrame):\n",
    "    \"\"\"DocString\"\"\"\n",
    "    # event_time\n",
    "    df[\"event_time\"] = pd.to_datetime(\n",
    "        df[\"event_time\"], utc=True, errors=\"coerce\")\n",
    "    df = df.dropna(subset=['event_time'])\n",
    "\n",
    "    # event_type\n",
    "    valid_events = [\"view\", \"cart\", \"purchase\", \"remove_from_cart\"]\n",
    "    df = df[df[\"event_type\"].isin(valid_events)]\n",
    "\n",
    "    # category_code to main_category and sub_category\n",
    "    df['category_code'] = df['category_code'].fillna('unknown')\n",
    "    df['category_code'] = df['category_code'].astype(str)\n",
    "    df['main_category'] = df['category_code'].apply(\n",
    "        lambda x: x.split('.')[0] if x != 'unknown' else 'unknown')\n",
    "    df['sub_category'] = df['category_code'].apply(\n",
    "        lambda x: 'unknown' if x == 'unknown' else '.'.join(x.split('.')[1:]) or x)\n",
    "\n",
    "    # user_id\n",
    "    df = df.dropna(subset=['user_id'])\n",
    "\n",
    "    # brand\n",
    "    df[\"brand\"] = df[\"brand\"].fillna(\"unknown\")\n",
    "    df[\"brand\"] = df[\"brand\"].astype(str)\n",
    "\n",
    "    # price\n",
    "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "    df = df[~((df['event_type'] == 'purchase') & (df['price'].isnull()))]\n",
    "    df['price'] = df['price'].fillna(0)\n",
    "    df['price'] = df['price'].astype(float)\n",
    "\n",
    "    # user_session\n",
    "    df['user_session'] = df['user_session'].fillna('unknown')\n",
    "    df['user_session'] = df['user_session'].astype(str)\n",
    "\n",
    "    # drop columns\n",
    "    df = df.drop(columns=[\"category_id\", \"category_code\", \"product_id\"])\n",
    "\n",
    "    return df.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
